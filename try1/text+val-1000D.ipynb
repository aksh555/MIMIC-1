{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "combined_dataset = pd.read_pickle('../combined_dataset_notes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45526, 504)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ADMISSION_DAYS</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>50801</th>\n",
       "      <th>50802</th>\n",
       "      <th>50803</th>\n",
       "      <th>50804</th>\n",
       "      <th>50805</th>\n",
       "      <th>50806</th>\n",
       "      <th>...</th>\n",
       "      <th>G11</th>\n",
       "      <th>G12</th>\n",
       "      <th>G13</th>\n",
       "      <th>G14</th>\n",
       "      <th>G15</th>\n",
       "      <th>G16</th>\n",
       "      <th>G17</th>\n",
       "      <th>G18</th>\n",
       "      <th>G19</th>\n",
       "      <th>G20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>47366</td>\n",
       "      <td>[chest, pa, lat, clip, clip, number, radiology...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47024</td>\n",
       "      <td>[pm, liver, gallbladder, u, single, organ, cli...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9046</td>\n",
       "      <td>[chest, portable, ap, clip, clip, number, radi...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>[pm, chest, pa, lat, clip, clip, number, radio...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>462.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43466</td>\n",
       "      <td>[pm, chest, portable, ap, different, physician...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 504 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  ADMISSION_DAYS  \\\n",
       "47366  [chest, pa, lat, clip, clip, number, radiology...               7   \n",
       "47024  [pm, liver, gallbladder, u, single, organ, cli...               5   \n",
       "9046   [chest, portable, ap, clip, clip, number, radi...              13   \n",
       "843    [pm, chest, pa, lat, clip, clip, number, radio...               5   \n",
       "43466  [pm, chest, portable, ap, different, physician...               5   \n",
       "\n",
       "       GENDER  AGE  50801  50802  50803  50804  50805  50806  ...  G11  G12  \\\n",
       "47366       0   35  462.0    0.0   24.0   26.0    1.0  104.0  ...    0    1   \n",
       "47024       1   60  462.0   -6.0   24.0   17.0    1.0  104.0  ...    0    1   \n",
       "9046        0   49  462.0    0.0   24.0   26.0    1.0  104.0  ...    0    0   \n",
       "843         1   61  462.0    2.0   24.0   28.0    1.0  103.0  ...    0    0   \n",
       "43466       0   54  462.0    0.0   23.0   25.0    1.0  106.0  ...    0    0   \n",
       "\n",
       "       G13  G14  G15  G16  G17  G18  G19  G20  \n",
       "47366    0    0    0    0    0    0    0    1  \n",
       "47024    0    0    0    1    0    0    0    0  \n",
       "9046     0    0    0    1    0    0    0    1  \n",
       "843      0    0    0    0    0    1    0    1  \n",
       "43466    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 504 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "combined_dataset = combined_dataset.drop(['HADM_ID'], axis=1)\n",
    "print(combined_dataset.shape)\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45526, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_text = combined_dataset[[\"text\",\"G1\",\"G2\",\"G3\",\"G4\",\"G5\",\"G6\",\"G7\",\"G8\",\"G9\",\"G10\",\"G11\",\"G12\",\"G13\",\"G14\",\"G15\",\"G16\",\"G17\",\"G18\",\"G19\",\"G20\"]]\n",
    "only_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /home/akshara/anaconda3/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/akshara/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/akshara/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.10.42)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/akshara/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.42 in /home/akshara/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.42)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/akshara/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/akshara/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.42->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /home/akshara/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.42->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already up-to-date: gensim in /home/akshara/anaconda3/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/akshara/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/akshara/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/akshara/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.10.42)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/akshara/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/akshara/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.42 in /home/akshara/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.42)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/akshara/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/akshara/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.42->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /home/akshara/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.42->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=23960, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#pass text here..\n",
    "#model = Word2Vec(combined_dataset.text,size=1000, window=10, min_count=5, workers=56)#play around with param\n",
    "# summarize the loaded model\n",
    "#print(model)\n",
    "# summarize vocabulary\n",
    "#words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "# access vector for one word\n",
    "#print(model['sentence'])\n",
    "# save model\n",
    "#model.save('model1000-10-5.bin')\n",
    "# load model\n",
    "model = Word2Vec.load('model300-10-5.bin')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = only_text.iloc[:,1:]\n",
    "x = only_text.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45526, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38697, 1) (6829, 1) (38697, 20) (6829, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed=243 \n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.15,random_state=seed)\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%matplotlib inline\\nfrom sklearn.decomposition import PCA\\nfrom matplotlib import pyplot\\nX = model[model.wv.vocab]\\npca = PCA(n_components=2)\\nresult = pca.fit_transform(X)\\n# create a scatter plot of the projection\\npyplot.scatter(result[:, 0], result[:, 1])\\nwords = list(model.wv.vocab)\\nfor i, word in enumerate(words):\\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\\npyplot.show()'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AverageEmbeddingVectorizer at 0x7f849ea4bf50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordVectorz=dict(zip(model.wv.index2word,model.wv.vectors))\n",
    "class AverageEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim =100 # because we use 100 embedding points \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "AverageEmbeddingVectorizer(WordVectorz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.metrics import jaccard_score,roc_auc_score,confusion_matrix,hamming_loss\n",
    "\n",
    "#clf_multilabel = OneVsRestClassifier(LGBMClassifier(iterations=1000,\n",
    "#                                                                     learning_rate=0.15))\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('wordVectz',\n",
       "                 <__main__.AverageEmbeddingVectorizer object at 0x7f5078bad750>),\n",
       "                ('multilabel',\n",
       "                 OneVsRestClassifier(estimator=LGBMClassifier(boosting_type='gbdt',\n",
       "                                                              class_weight=None,\n",
       "                                                              colsample_bytree=1.0,\n",
       "                                                              importance_type='split',\n",
       "                                                              iterations=1000,\n",
       "                                                              learning_rate=0.15,\n",
       "                                                              max_depth=-1,\n",
       "                                                              min_child_samples=20,\n",
       "                                                              min_child_weight=0.001,\n",
       "                                                              min_split_gain=0.0,\n",
       "                                                              n_estimators=100,\n",
       "                                                              n_jobs=-1,\n",
       "                                                              num_leaves=31,\n",
       "                                                              objective=None,\n",
       "                                                              random_state=None,\n",
       "                                                              reg_alpha=0.0,\n",
       "                                                              reg_lambda=0.0,\n",
       "                                                              silent=True,\n",
       "                                                              subsample=1.0,\n",
       "                                                              subsample_for_bin=200000,\n",
       "                                                              subsample_freq=0),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pipe1=Pipeline([(\"wordVectz\",AverageEmbeddingVectorizer(WordVectorz)),(\"multilabel\",OneVsRestClassifier(LGBMClassifier(iterations=1000,learning_rate=0.15,n_jobs=-1))),\n",
    "               (\"multilabel2\",OneVsRestClassifier(LGBMClassifier(iterations=1500,learning_rate=0.15,n_jobs=-1)))])\n",
    "pipe1.fit(x_train.text,y_train)#run this and see but not here lets do this in a new nb\n",
    "# clf_multilabel.fit(x_train.text,y_train)#have to now oopsitsk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1=pipe1.predict(x_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1_prob_train = pipe1.predict_proba(x_train.text)\n",
    "predicted1_prob_test = pipe1.predict_proba(x_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5034983002608902\n",
      "0.39044713301507433\n",
      "0.6646507472338611\n",
      "0.1839288329184361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCat w2v 500,10,5\\n0.6633380308436122\\n0.18253038512227265'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(jaccard_score(y_test,predicted1,average='micro'))\n",
    "print(jaccard_score(y_test,predicted1,average='macro'))\n",
    "print(roc_auc_score(y_test,predicted1))\n",
    "print(hamming_loss(y_test,predicted1))\n",
    "'''\n",
    "Cat w2v 500,10,5\n",
    "0.6633380308436122\n",
    "0.18253038512227265'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6829, 19)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted1.shape #earlier with 1500d & mincount=3 - 0.6573500885474709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe1.fit(x_train.text,y_train)\n",
    "predicted1_prob_testdf = pd.DataFrame(predicted1_prob_test)\n",
    "predicted1_prob_traindf = pd.DataFrame(predicted1_prob_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6829, 20)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted1_prob_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>0.381461</td>\n",
       "      <td>0.244952</td>\n",
       "      <td>0.226117</td>\n",
       "      <td>0.133170</td>\n",
       "      <td>0.422299</td>\n",
       "      <td>0.114607</td>\n",
       "      <td>0.154242</td>\n",
       "      <td>0.135912</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.029295</td>\n",
       "      <td>0.206063</td>\n",
       "      <td>0.008920</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.144964</td>\n",
       "      <td>0.027488</td>\n",
       "      <td>0.984861</td>\n",
       "      <td>0.983045</td>\n",
       "      <td>0.347243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060448</td>\n",
       "      <td>0.020596</td>\n",
       "      <td>0.792334</td>\n",
       "      <td>0.469381</td>\n",
       "      <td>0.554800</td>\n",
       "      <td>0.337727</td>\n",
       "      <td>0.471109</td>\n",
       "      <td>0.146749</td>\n",
       "      <td>0.755750</td>\n",
       "      <td>0.457869</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.103819</td>\n",
       "      <td>0.162318</td>\n",
       "      <td>0.017451</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.527361</td>\n",
       "      <td>0.014730</td>\n",
       "      <td>0.352813</td>\n",
       "      <td>0.349358</td>\n",
       "      <td>0.289949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.047712</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>0.111793</td>\n",
       "      <td>0.401464</td>\n",
       "      <td>0.651509</td>\n",
       "      <td>0.855404</td>\n",
       "      <td>0.151393</td>\n",
       "      <td>0.127526</td>\n",
       "      <td>0.281706</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.029521</td>\n",
       "      <td>0.174612</td>\n",
       "      <td>0.006954</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.310654</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.515857</td>\n",
       "      <td>0.806501</td>\n",
       "      <td>0.454475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>0.778939</td>\n",
       "      <td>0.136471</td>\n",
       "      <td>0.182365</td>\n",
       "      <td>0.104084</td>\n",
       "      <td>0.989231</td>\n",
       "      <td>0.141261</td>\n",
       "      <td>0.191664</td>\n",
       "      <td>0.090097</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.020778</td>\n",
       "      <td>0.120590</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.082227</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.168287</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.452310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.155675</td>\n",
       "      <td>0.350358</td>\n",
       "      <td>0.850952</td>\n",
       "      <td>0.722703</td>\n",
       "      <td>0.364650</td>\n",
       "      <td>0.714532</td>\n",
       "      <td>0.905861</td>\n",
       "      <td>0.852393</td>\n",
       "      <td>0.701567</td>\n",
       "      <td>0.771462</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.397246</td>\n",
       "      <td>0.220130</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.672055</td>\n",
       "      <td>0.041518</td>\n",
       "      <td>0.754205</td>\n",
       "      <td>0.359311</td>\n",
       "      <td>0.675329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6824</td>\n",
       "      <td>0.095784</td>\n",
       "      <td>0.137530</td>\n",
       "      <td>0.860846</td>\n",
       "      <td>0.712012</td>\n",
       "      <td>0.310655</td>\n",
       "      <td>0.738019</td>\n",
       "      <td>0.927311</td>\n",
       "      <td>0.315240</td>\n",
       "      <td>0.471653</td>\n",
       "      <td>0.871125</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.494851</td>\n",
       "      <td>0.312031</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.528199</td>\n",
       "      <td>0.029597</td>\n",
       "      <td>0.859444</td>\n",
       "      <td>0.631167</td>\n",
       "      <td>0.771659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6825</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.068985</td>\n",
       "      <td>0.846435</td>\n",
       "      <td>0.632877</td>\n",
       "      <td>0.799036</td>\n",
       "      <td>0.569135</td>\n",
       "      <td>0.780485</td>\n",
       "      <td>0.465946</td>\n",
       "      <td>0.804712</td>\n",
       "      <td>0.705267</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.456766</td>\n",
       "      <td>0.196635</td>\n",
       "      <td>0.019320</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.601936</td>\n",
       "      <td>0.041650</td>\n",
       "      <td>0.657041</td>\n",
       "      <td>0.343640</td>\n",
       "      <td>0.647979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6826</td>\n",
       "      <td>0.042937</td>\n",
       "      <td>0.040011</td>\n",
       "      <td>0.640910</td>\n",
       "      <td>0.506048</td>\n",
       "      <td>0.862476</td>\n",
       "      <td>0.538513</td>\n",
       "      <td>0.452046</td>\n",
       "      <td>0.583061</td>\n",
       "      <td>0.328257</td>\n",
       "      <td>0.358777</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.209272</td>\n",
       "      <td>0.294012</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.636439</td>\n",
       "      <td>0.031056</td>\n",
       "      <td>0.493248</td>\n",
       "      <td>0.481291</td>\n",
       "      <td>0.412246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6827</td>\n",
       "      <td>0.105662</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>0.846555</td>\n",
       "      <td>0.791480</td>\n",
       "      <td>0.317569</td>\n",
       "      <td>0.542417</td>\n",
       "      <td>0.959383</td>\n",
       "      <td>0.258478</td>\n",
       "      <td>0.417617</td>\n",
       "      <td>0.915276</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.367771</td>\n",
       "      <td>0.196251</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.452880</td>\n",
       "      <td>0.026391</td>\n",
       "      <td>0.855679</td>\n",
       "      <td>0.562148</td>\n",
       "      <td>0.643258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6828</td>\n",
       "      <td>0.013077</td>\n",
       "      <td>0.065920</td>\n",
       "      <td>0.731566</td>\n",
       "      <td>0.123670</td>\n",
       "      <td>0.108444</td>\n",
       "      <td>0.115051</td>\n",
       "      <td>0.989617</td>\n",
       "      <td>0.237605</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.136465</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.049520</td>\n",
       "      <td>0.122557</td>\n",
       "      <td>0.061927</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.071558</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.372009</td>\n",
       "      <td>0.153516</td>\n",
       "      <td>0.321672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6829 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.006364  0.014811  0.381461  0.244952  0.226117  0.133170  0.422299   \n",
       "1     0.060448  0.020596  0.792334  0.469381  0.554800  0.337727  0.471109   \n",
       "2     0.011747  0.047712  0.474600  0.111793  0.401464  0.651509  0.855404   \n",
       "3     0.009529  0.030799  0.778939  0.136471  0.182365  0.104084  0.989231   \n",
       "4     0.155675  0.350358  0.850952  0.722703  0.364650  0.714532  0.905861   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6824  0.095784  0.137530  0.860846  0.712012  0.310655  0.738019  0.927311   \n",
       "6825  0.070175  0.068985  0.846435  0.632877  0.799036  0.569135  0.780485   \n",
       "6826  0.042937  0.040011  0.640910  0.506048  0.862476  0.538513  0.452046   \n",
       "6827  0.105662  0.080924  0.846555  0.791480  0.317569  0.542417  0.959383   \n",
       "6828  0.013077  0.065920  0.731566  0.123670  0.108444  0.115051  0.989617   \n",
       "\n",
       "             7         8         9        10        11        12        13  \\\n",
       "0     0.114607  0.154242  0.135912  0.000083  0.029295  0.206063  0.008920   \n",
       "1     0.146749  0.755750  0.457869  0.001473  0.103819  0.162318  0.017451   \n",
       "2     0.151393  0.127526  0.281706  0.000056  0.029521  0.174612  0.006954   \n",
       "3     0.141261  0.191664  0.090097  0.000048  0.020778  0.120590  0.015656   \n",
       "4     0.852393  0.701567  0.771462  0.001481  0.397246  0.220130  0.035454   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6824  0.315240  0.471653  0.871125  0.000228  0.494851  0.312031  0.008207   \n",
       "6825  0.465946  0.804712  0.705267  0.001412  0.456766  0.196635  0.019320   \n",
       "6826  0.583061  0.328257  0.358777  0.000726  0.209272  0.294012  0.011878   \n",
       "6827  0.258478  0.417617  0.915276  0.000217  0.367771  0.196251  0.009987   \n",
       "6828  0.237605  0.191000  0.136465  0.000037  0.049520  0.122557  0.061927   \n",
       "\n",
       "            14        15        16        17        18        19  \n",
       "0     0.000142  0.144964  0.027488  0.984861  0.983045  0.347243  \n",
       "1     0.000144  0.527361  0.014730  0.352813  0.349358  0.289949  \n",
       "2     0.000131  0.310654  0.001712  0.515857  0.806501  0.454475  \n",
       "3     0.000347  0.082227  0.001190  0.168287  0.069491  0.452310  \n",
       "4     0.000101  0.672055  0.041518  0.754205  0.359311  0.675329  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "6824  0.000102  0.528199  0.029597  0.859444  0.631167  0.771659  \n",
       "6825  0.000352  0.601936  0.041650  0.657041  0.343640  0.647979  \n",
       "6826  0.000791  0.636439  0.031056  0.493248  0.481291  0.412246  \n",
       "6827  0.000130  0.452880  0.026391  0.855679  0.562148  0.643258  \n",
       "6828  0.000110  0.071558  0.001343  0.372009  0.153516  0.321672  \n",
       "\n",
       "[6829 rows x 20 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted1_prob_testdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train.text.to_pickle('./train-nlp.pkl')\n",
    "x_test.text.to_pickle('./test-nlp.pkl')\n",
    "y_train.to_pickle('./y_train-nlp.pkl')\n",
    "y_test.to_pickle('./y_test-nlp.pkl')\n",
    "predicted1_prob_testdf.to_pickle('./pred-test-xgb.pkl')\n",
    "predicted1_prob_traindf.to_pickle('./pred-train-xgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipe1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3f46062c396f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w2v_lgbm.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipe1, \"w2v_lgbm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM text\n",
    "textlgbm =  joblib.load(\"w2v_lgbm.pkl\")\n",
    "nb_struc  = joblib.load(\"NB-struc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#valc = joblib.load(\"cat1-C.pkl\")\n",
    "#textc = joblib.load(\"./w2v.pkl\")\n",
    "lgbm = joblib.load(\"./LGBM1-C.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = combined_dataset.iloc[:,:484]\n",
    "y_full = combined_dataset.iloc[:,484:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([          'text', 'ADMISSION_DAYS',         'GENDER',            'AGE',\n",
       "                  50801,            50802,            50803,            50804,\n",
       "                  50805,            50806,\n",
       "       ...\n",
       "                  51512,            51513,            51514,            51515,\n",
       "                  51516,            51517,            51518,            51519,\n",
       "                  51529,            51532],\n",
       "      dtype='object', length=484)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADMISSION_DAYS</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>50801</th>\n",
       "      <th>50802</th>\n",
       "      <th>50803</th>\n",
       "      <th>50804</th>\n",
       "      <th>50805</th>\n",
       "      <th>50806</th>\n",
       "      <th>50808</th>\n",
       "      <th>...</th>\n",
       "      <th>51512</th>\n",
       "      <th>51513</th>\n",
       "      <th>51514</th>\n",
       "      <th>51515</th>\n",
       "      <th>51516</th>\n",
       "      <th>51517</th>\n",
       "      <th>51518</th>\n",
       "      <th>51519</th>\n",
       "      <th>51529</th>\n",
       "      <th>51532</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21738</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>462.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47854</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3615</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>462.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37273</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12132</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>338.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5542</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>584.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57534</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40358</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24988</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>391.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28595</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6829 rows × 483 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ADMISSION_DAYS  GENDER  AGE  50801  50802  50803  50804  50805  50806  \\\n",
       "21738              10       1   83  462.0    5.0   24.0   32.0    1.0  104.0   \n",
       "47854               3       1   66  462.0   -2.0   22.0   22.0    1.0  105.0   \n",
       "3615                8       1   80  462.0    1.0   24.0   26.0    1.0  104.0   \n",
       "37273               5       1   46  462.0    0.0   24.0   26.0    1.0  104.0   \n",
       "12132              27       1   85  338.0   -1.0   24.0   25.0    1.0  100.0   \n",
       "...               ...     ...  ...    ...    ...    ...    ...    ...    ...   \n",
       "5542               16       1   82  584.0    0.0   24.0   26.0    1.0  104.0   \n",
       "57534              13       1   56  462.0    0.0   24.0   29.0    1.0  102.0   \n",
       "40358              12       1   36  462.0    0.0   24.0   26.0    1.0  104.0   \n",
       "24988              14       1   50  391.0    6.0   24.0   32.0    1.0  101.0   \n",
       "28595              17       0    0  462.0   -5.0   24.0   23.0    1.0  104.0   \n",
       "\n",
       "       50808  ...  51512  51513  51514  51515  51516  51517  51518  51519  \\\n",
       "21738   1.12  ...    1.0  1.044    1.0    1.0    2.0    3.0    0.0    0.0   \n",
       "47854   1.10  ...    1.0  1.044    1.0    1.0    2.0    3.0    0.0    0.0   \n",
       "3615    1.15  ...    1.0  1.044    1.0    1.0    2.0    3.0    0.0    0.0   \n",
       "37273   1.12  ...    1.0  1.044    4.0    1.0    2.0    3.0    0.0    0.0   \n",
       "12132   1.14  ...    1.0  1.044    0.2    1.0    0.0    3.0    0.0    0.0   \n",
       "...      ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5542    1.12  ...    1.0  1.044    0.2    1.0    7.0    3.0    0.0    0.0   \n",
       "57534   1.25  ...    1.0  1.044    1.0    1.0    2.0    3.0    0.0    0.0   \n",
       "40358   1.12  ...    1.0  1.044    1.0    1.0    2.0    3.0    0.0    0.0   \n",
       "24988   1.18  ...    1.0  1.044    0.2    1.0    0.0    3.0    0.0    0.0   \n",
       "28595   1.12  ...    1.0  1.044    1.0    1.0    2.0    3.0    0.0    0.0   \n",
       "\n",
       "       51529  51532  \n",
       "21738  135.5   43.0  \n",
       "47854  135.5   43.0  \n",
       "3615   135.5   43.0  \n",
       "37273  135.5   43.0  \n",
       "12132  135.5   43.0  \n",
       "...      ...    ...  \n",
       "5542   135.5   43.0  \n",
       "57534  135.5   43.0  \n",
       "40358  135.5   43.0  \n",
       "24988  135.5   43.0  \n",
       "28595  135.5   43.0  \n",
       "\n",
       "[6829 rows x 483 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed=243 \n",
    "x_train_full,x_test_full,y_train_full,y_test_full = train_test_split(x_full,y_full,test_size=0.15,random_state=seed)\n",
    "x_test_full.iloc[:,1:484]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38697, 20) (38697, 20)\n"
     ]
    }
   ],
   "source": [
    "y_pred_catnum = valc.predict(x_train_full.iloc[:,1:484])\n",
    "y_pred_catext = textc.predict(x_train_full.text)\n",
    "\n",
    "print(y_pred_catnum.shape,y_pred_catext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6829, 20)\n"
     ]
    }
   ],
   "source": [
    "y_pred_lg = lgbm.predict(x_test_full.iloc[:,1:484])\n",
    "y_pred_lg_prob = lgbm.predict_proba(x_test_full.iloc[:,1:484])\n",
    "print(y_pred_lg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6829, 20)\n"
     ]
    }
   ],
   "source": [
    "y_pred_lgbm = textlgbm.predict(x_test_full.text)#need str stuff ? this is text only, we were using lgbm for str and cat for text\n",
    "y_pred_lgbm_prob = textlgbm.predict_proba(x_test_full.text)\n",
    "\n",
    "print(y_pred_lgbm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6829, 20)\n"
     ]
    }
   ],
   "source": [
    "y_pred_lgbm_text = textlgbm.predict(x_test_full.text)\n",
    "y_pred_lgbm_text_prob = textlgbm.predict_proba(x_test_full.text)\n",
    "\n",
    "print(y_pred_lgbm_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6829, 20) (6829, 20)\n"
     ]
    }
   ],
   "source": [
    "y_pred_catnum_prob = valc.predict_proba(x_test_full.iloc[:,1:484])\n",
    "y_pred_catext_prob = textc.predict_proba(x_test_full.text)\n",
    "print(y_pred_catnum.shape,y_pred_catext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nb_prob = nb_struc.predict_proba(x_test_full.iloc[:,1:484])\n",
    "y_nb = nb_struc.predict(x_test_full.iloc[:,1:484])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what all is needed lol \n",
    "# i need lgbm for str and cat for text on train or test or both? both na?\n",
    "y_pred_lg_prob_train = lgbm.predict_proba(x_train_full.iloc[:,1:484])\n",
    "y_pred_lg_prob_test = lgbm.predict_proba(x_test_full.iloc[:,1:484])\n",
    "y_pred_catext_prob_train = textc.predict_proba(x_train_full.text)\n",
    "y_pred_catext_prob_test = textc.predict_proba(x_test_full.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_comb =y_nb_prob+y_pred_lgbm_prob\n",
    "y_comb/=2 #shouldn't u round the other one also??\n",
    "y_comb = y_comb.round()\n",
    "#y_comb = np.where(y_comb>1,1,0)#wait u got this wrong\n",
    "#just try this, according to me text can give more info, and i would change w2v to 1000 \n",
    "#we get max only when u put lg as 1 and the other as 0, haan but loss should be optimum..so kinda mid, this works??\n",
    "#0.7668781779155269\n",
    "#0.17367843022404453\n",
    "#w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.753483427059962\n",
    "0.15721921218333576 for .72&.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21608533119130005\n",
      "0.20516881912755008\n",
      "0.5003895995388028\n",
      "0.7431175867623371\n"
     ]
    }
   ],
   "source": [
    "# predicting on avg of both probs on test set\n",
    "print(jaccard_score(y_test,y_comb,average='micro'))\n",
    "print(jaccard_score(y_test,y_comb,average='macro'))\n",
    "print(roc_auc_score(y_test,y_comb))\n",
    "print(hamming_loss(y_test,y_comb))\n",
    "#0.6646507472338611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6937055799183427\n",
      "0.648084064314032\n",
      "0.8135841276855682\n",
      "0.10468640979920923\n"
     ]
    }
   ],
   "source": [
    "# predicting on train set\n",
    "print(jaccard_score(y_train_full,y_comb,average='micro'))\n",
    "print(jaccard_score(y_train_full,y_comb,average='macro'))\n",
    "print(roc_auc_score(y_train_full,y_comb))\n",
    "print(hamming_loss(y_train_full,y_comb))\n",
    "#0.6646507472338611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_sum = 0.7664439688584423/0.17394371656717575 + 0.8135841276855682/0.10468640979920923\n",
    "#auc/ham worked...\n",
    "y_comb = (y_pred_lg_prob_test*0.7664439688584423/0.17394371656717575 +y_pred_catext_prob_test*0.8135841276855682/0.10468640979920923)/auc_sum\n",
    "y_comb = y_comb.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def corr(a, b):\n",
    "    counts = 0\n",
    "  #first_df = pd.read_csv(first_file,index_col=0)\n",
    "  #second_df = pd.read_csv(second_file,index_col=0)\n",
    "  # assuming first column is `prediction_id` and second column is `prediction`\n",
    "    for i in range(a.shape[0]):  \n",
    "      #prediction = a[:,i] #first_df.columns[0]\n",
    "      # correlation\n",
    "        #print(\"Finding correlation row : {}\".format(i))\n",
    "      #print(\"Column to be measured: {}\".format(prediction))\n",
    "        cor = np.corrcoef(a[i,:],b[i,:])\n",
    "        #print(\"Pearson's correlation score: {}\".format(cor))\n",
    "        if(cor[0][1] < 0.7):\n",
    "            counts+=1\n",
    "      #print(\"Kendall's correlation score: {}\".format(first_df[prediction].corr(second_df[prediction],method='kendall')))\n",
    "      #print(\"Spearman's correlation score: {}\".format(first_df[prediction].corr(second_df[prediction],method='spearman')))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[521]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [corr(y_pred_catnum,y_pred_lg)]\n",
    "l# this was high thats y its working better ig\n",
    "#what was high? like correlation was low for many inspite o\n",
    "# 4717 cat v cat\n",
    "# 4909 lgbm vs cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators=[ ('text', textc), ('val', valc)]\n",
    "ensemble = OneVsRestClassifier(VotingClassifier(estimators, voting='soft'))\n",
    "ensemble.fit(x_train_mm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mimic)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
